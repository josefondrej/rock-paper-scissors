{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car from OpenAI Gym\n",
    "\n",
    "Rewritten code from https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c \n",
    "\n",
    "Basic Deep-Q idea: \n",
    "\n",
    "- initialize replay memory\n",
    "- initializer action-value function $Q$ with random weights \n",
    "- observe initial state $s$ \n",
    "- select an action a -- with proba $\\varepsilon$ random otherwise $a = argmax_{a^{\\prime}} Q(s, a^{\\prime})$\n",
    "- carry out $a$\n",
    "- observe reward $r$ and new state $s^{\\prime}$\n",
    "- store experience <$s,a,r,s^{\\prime}$> into replay memory\n",
    "    \n",
    "- sample random transitions <$ss,aa,rr,ss^{\\prime}$> from replay memory\n",
    "- calculate target for each minibatch transition - if $ss^{\\prime}$ is a terminal state then $tt = rr$ otherwise $tt = rr + \\gamma max_{a^{\\prime}}Q(ss^{\\prime}, a^{\\prime})$\n",
    "- train the Q function using $(tt - Q(ss, aa))^{2}$ as loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init and Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VlZ6838UW24y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import Sequential \n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1524689747104,
     "user": {
      "displayName": "Josef Ond≈ôej",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117870852030343720634"
     },
     "user_tz": -120
    },
    "id": "4EtZwcdzW4pg",
    "outputId": "2f36e937-adb3-482c-ed3e-adf5c987cabf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI gym mountain car environment \n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "n_in = env.observation_space.shape[0]\n",
    "n_out = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rODUQy8rYkND"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005       # lr for GD optimizer of the Q function\n",
    "memory_len = 2000           # replay memory size  \n",
    "max_iter = 10000            # how many times to iterate (1 iter = 200 pushes or car successfully got to the top)\n",
    "epsilon = 1.0               # exploitation / exploration ratio\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.998\n",
    "batch_size = 32             # number of samples to draw from replay memory used for training \n",
    "gamma = 0.85                # time discount factor \n",
    "tau = 0.125                 # updating factor for the weights of the target model from prediction model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pbQq4t2LXnTq"
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation=\"relu\", input_shape = (2 * n_in,)))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(n_out, activation=\"relu\"))\n",
    "    model.compile(loss=\"mse\", optimizer = Adam(lr = learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F4aDF5shZ7uF"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "target_model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92kue7_1YflD"
   },
   "outputs": [],
   "source": [
    "memory = deque(maxlen = memory_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zfpArPBkZCqI",
    "outputId": "1835ac54-ce3c-4eb4-ee64-6c3e758270c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/100000 [00:30<52:22:04,  1.89s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(max_iter)): \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    s = np.concatenate([obs, obs]).reshape(1,2*n_in)\n",
    "  \n",
    "    while not done: \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Select action, execute it and store result in memory \n",
    "        if np.random.uniform() < epsilon: \n",
    "            a = env.action_space.sample()\n",
    "        else: \n",
    "            a = np.argmax(model.predict(s)[0])\n",
    "      \n",
    "        obs, reward, done, _ = env.step(a)\n",
    "        s_new = np.concatenate([s[0], obs])[n_in:].reshape(1,2*n_in) \n",
    "        memory.append([s, a, reward, s_new, done]) \n",
    "        total_reward += reward\n",
    "    \n",
    "        # Update Q from replay \n",
    "        if 200 * i > batch_size: \n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, states_new = [np.concatenate([b[i] for b in batch]) for i in [0, 3]]\n",
    "            actions, rewards, dones = [np.array([b[i] for b in batch]) for i in [1, 2, 4]]\n",
    "    \n",
    "            max_Q = np.max(model_target.predict(states_new), axis = 1)\n",
    "    \n",
    "            targets = model.predict(states)\n",
    "            targets[range(batch_size), actions] = rewards + (1-dones) * gamma * max_Q    \n",
    "            model.fit(states, targets, verbose = 0)    \n",
    "             \n",
    "            weights = model.get_weights()\n",
    "            target_weights = target_model.get_weights()\n",
    "            for i in range(len(target_weights)):\n",
    "                target_weights[i] = weights[i] * tau + target_weights[i] * (1 - tau)\n",
    "            target_model.set_weights(target_weights)\n",
    "\n",
    "        if total_reward > -200: \n",
    "        print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My Mountain Car Agent.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
